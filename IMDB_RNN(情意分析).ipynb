{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Data Analysis Based on RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%env KERAS_BACKEND = tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from keras.constraints import max_norm\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把文字的種類限制到 10000 字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看一下 training data 和 testing data 的 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training data: (25000,); shape of testing data: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(f'shape of training data: {x_train.shape}; shape of testing data: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把 training 和 testing input 的字數都刪減或增補到 300 字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_seq = sequence.pad_sequences(x_train, maxlen = 300)\n",
    "x_test_seq = sequence.pad_sequences(x_test, maxlen = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看一下被動過手腳之後的 input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training data: (25000, 300); shape of testing data: (25000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(f'shape of training data: {x_train_seq.shape}; shape of testing data: {x_test_seq.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1<sup>st</sup>\n",
    "### 1 層 Embedding + 1 層 LSTM + 1 層  Dense 的神經網路\n",
    "之前試過好多參數，測試資料準確率都不超過 86% ，參考學弟致元的做法及建議，改用 nadam 當 optimizer， N 跟 K 從 (8,5) 改大到 (320,16) ，測試資料的準確率好不容易來到 86.80 % 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 320)         3200000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                21568     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,221,585\n",
      "Trainable params: 3,221,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "N = 320# 把10000壓到 N 維\n",
    "K = 16 # LSTM 有 K 個神經元\n",
    "\n",
    "model_1st = Sequential()\n",
    "model_1st.add(Embedding(10000,N))\n",
    "model_1st.add(LSTM(K))\n",
    "model_1st.add(Dense(1,activation='sigmoid'))\n",
    "model_1st.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "model_1st.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就如各位同學所指出的，這次的訓練資料很容易產生 overfitting，所以在 fit 的參數裡面設了 early stopping 的 callback, 採用 testing data 做 validation_data, 讓結果能收在 testing data 準確率最高的權重 (有開 restore_best_weights= True)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 131s 5ms/step - loss: 0.4884 - acc: 0.7653 - val_loss: 0.4682 - val_acc: 0.8032\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 125s 5ms/step - loss: 0.3157 - acc: 0.8748 - val_loss: 0.3683 - val_acc: 0.8421\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 125s 5ms/step - loss: 0.2230 - acc: 0.9174 - val_loss: 0.3323 - val_acc: 0.8680\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 124s 5ms/step - loss: 0.1739 - acc: 0.9368 - val_loss: 0.3629 - val_acc: 0.8668\n"
     ]
    }
   ],
   "source": [
    "#model_1st.load_weights('./RNN_IMDB_Machines/model_1st_weights.h5')\n",
    "\n",
    "model_1st_early = EarlyStopping(monitor='val_acc', min_delta=0, patience=0, verbose=0, mode='max', restore_best_weights=True)\n",
    "model_1st_history = model_1st.fit(x_train_seq,y_train, batch_size=256, epochs=5, validation_data=(x_test_seq, y_test), callbacks = [model_1st_early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其實可以不用做這步，因為上面訓練的時候validation_data 已經餵 testing data 進去了，直接看 valid_acc 就好~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 54s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3323085970878601, 0.86804]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1st_score = model_1st.evaluate(x_test_seq,y_test)\n",
    "model_1st_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The arrucay for 1st model is 86.80% ; loss is 0.3323 .\n"
     ]
    }
   ],
   "source": [
    "print(f'The arrucay for 1st model is {100*model_1st_score[1]:.2f}% ; loss is {model_1st_score[0]:.4f} .' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_1st_json = model_1st.to_json()\n",
    "#open('./RNN_IMDB_Machines/model_1st','w').write(model_1st_json)\n",
    "#model_1st.save_weights('./RNN_IMDB_Machines/model_1st_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 <sup>nd</sup>\n",
    "### 用 N=8 及 LSTM Cell = 5，加上 Dropout 以及權重限制 Constraint，設計新的 RNN 結構: 1 層 Embedding + Dropout +1 層 LSTM + Dropout + 1 層  Dense\n",
    "對 LSTM Cell ，能對 kernel, recurrent, 以及 bias 做限制，值在3.0以下; 對 Dense 層，限制 kernel, bias 在2.0 以下。 Dropout 一開始採建議的 0.2-0.5 之間，發現效果沒很好，所以開始亂試畸形的數值，最後發現第一層 Dropout 用低一點效果不錯。用 Dropout 以及 Constraint 減緩 overfitting 後，測試資料準確率能達到 86.1 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 8)           80000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 8)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5)                 280       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 80,286\n",
      "Trainable params: 80,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "N = 8 # 把10000壓到 N 維\n",
    "K = 5 # LSTM 有 K 個神經元\n",
    "\n",
    "model_2nd = Sequential()\n",
    "model_2nd.add(Embedding(10000,N))\n",
    "model_2nd.add(Dropout(0.75))\n",
    "model_2nd.add(LSTM(K, kernel_constraint=max_norm(3.0),recurrent_constraint=max_norm(3.0), bias_constraint=max_norm(3.0)))\n",
    "model_2nd.add(Dropout(0.2))\n",
    "model_2nd.add(Dense(1,activation='sigmoid',kernel_constraint=max_norm(2.0),bias_constraint=max_norm(2.0) ))\n",
    "model_2nd.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "model_2nd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 22s 874us/step - loss: 0.6230 - acc: 0.6530 - val_loss: 0.4871 - val_acc: 0.7900\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 18s 705us/step - loss: 0.4685 - acc: 0.8160 - val_loss: 0.4204 - val_acc: 0.8338\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 22s 873us/step - loss: 0.4262 - acc: 0.8355 - val_loss: 0.4007 - val_acc: 0.8406\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 24s 942us/step - loss: 0.4031 - acc: 0.8507 - val_loss: 0.3701 - val_acc: 0.8611\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 21s 841us/step - loss: 0.3795 - acc: 0.8630 - val_loss: 0.4605 - val_acc: 0.7940\n"
     ]
    }
   ],
   "source": [
    "model_2nd_early = EarlyStopping(monitor='val_acc', min_delta=0, patience=0, verbose=0, mode='max', restore_best_weights=True)\n",
    "model_2nd_history = model_2nd.fit(x_train_seq,y_train, batch_size=256, epochs=5,validation_data=(x_test_seq, y_test), callbacks = [model_2nd_early] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 16s 637us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3701012341213226, 0.86108]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2nd_score = model_2nd.evaluate(x_test_seq,y_test)\n",
    "model_2nd_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_2nd_json = model_2nd.to_json()\n",
    "#open('./RNN_IMDB_Machines/model_2nd','w').write(model_2nd_json)\n",
    "#model_2nd.save_weights('./RNN_IMDB_Machines/model_2nd_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The arrucay for 2nd model is 86.11% ; loss is 0.3701 .\n"
     ]
    }
   ],
   "source": [
    "print(f'The arrucay for 2nd model is {100*model_2nd_score[1]:.2f}% ; loss is {model_2nd_score[0]:.4f} .' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3<sup>rd</sup>: 最後輸出 one-hot encode 的 output ，所以最後 Dense 層有兩個神經元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "想說因為結果是想要區別評論是正面或負面，有點像分類，所以把 y_train 跟 y_test one-hot encoding 後不知道隊訓練有沒有幫助，所以試做這個模型。<br><br>\n",
    "先前在測試的時候用同樣的 RNN 結構，可是 maxlen = 150， Dropout 的參數略為不同(但同樣是第一層 0.7-0.75，第二層在 0.2 以下, optimizer 用 adam ，權重的 constraint 忘記了，不過都是在 3.0 以下 XD)，試出一組本來測試資料準確率大約 86.3%左右的神經網路。用這個網路的權重當基礎，經過一次把第一層 Dropout 設成 0.75 ，第二層設成 0.25 ，optimizer 改 nadam ，加上 early stopping 的訓練，測試資料準確率約來到 88.1%; 再用第一層 Dropout=0.7，第二層 Dropout= 0.2， optimizer 用 nadam 再訓練一次，準確率略為上升到 88.4% 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = np_utils.to_categorical(y_train,2)\n",
    "y_test_one_hot = np_utils.to_categorical(y_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, None, 8)           80000     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, None, 8)           0         \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 5)                 280       \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 12        \n",
      "=================================================================\n",
      "Total params: 80,292\n",
      "Trainable params: 80,292\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "N = 8 # 把10000壓到 N 維\n",
    "K = 5 # LSTM 有 K 個神經元\n",
    "\n",
    "model_3rd = Sequential()\n",
    "model_3rd.add(Embedding(10000,N))\n",
    "model_3rd.add(Dropout(0.7))\n",
    "model_3rd.add(LSTM(K, kernel_constraint=max_norm(3.0),recurrent_constraint=max_norm(3.0), bias_constraint=max_norm(3.0)))\n",
    "model_3rd.add(Dropout(0.2))\n",
    "model_3rd.add(Dense(2,activation='sigmoid',kernel_constraint=max_norm(2.0),bias_constraint=max_norm(2.0)))\n",
    "model_3rd.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "model_3rd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 21s 827us/step - loss: 0.3112 - acc: 0.8936 - val_loss: 0.3119 - val_acc: 0.8843\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 17s 700us/step - loss: 0.2967 - acc: 0.9015 - val_loss: 0.3192 - val_acc: 0.8795\n"
     ]
    }
   ],
   "source": [
    "model_3rd.load_weights('./RNN_IMDB_Machines/model_3rd_weights.h5')\n",
    "\n",
    "model_3rd_early = EarlyStopping(monitor='val_acc', min_delta=0, patience=0, verbose=0, mode='max', restore_best_weights=True)\n",
    "model_3rd_history = model_3rd.fit(x_train_seq,y_train_one_hot, batch_size=256, epochs=10,validation_data=(x_test_seq, y_test_one_hot), callbacks = [model_3rd_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 17s 660us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3118935162830353, 0.88426]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3rd_score = model_3rd.evaluate(x_test_seq,y_test_one_hot)\n",
    "model_3rd_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_3rd_json = model_3rd.to_json()\n",
    "#open('./RNN_IMDB_Machines/model_3rd','w').write(model_3rd_json)\n",
    "#model_3rd.save_weights('./RNN_IMDB_Machines/model_3rd_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The arrucay for 3rd model is 88.43% ; loss is 0.3119 .\n"
     ]
    }
   ],
   "source": [
    "print(f'The arrucay for 3rd model is {100*model_3rd_score[1]:.2f}% ; loss is {model_3rd_score[0]:.4f} .' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
